totalArea = vector()
totalPop = vector()
popDens = vector()
for(i in 1:length(city)){
tryCatch(getLocation(city[i],state[i]), error = print("no location"))
location <- getLocation(city[i],state[i])
geo <- rbind(geo,getGEO(location[grepl('Coord', location[,2]),1]))
stats = location[grepl('km2)', location[,2]),]
totalPop = rbind(totalPop,as.numeric(gsub(",", "", location[grepl('[:digits{0-9}:],[:digits{0-9}:]', location[,2]),2][1])))
print(totalPop[i])
totalArea <- rbind(totalArea,as.numeric(gsub(",", "", unlist(strsplit(stats[grepl('Total',stats[,1]),2],'sq'))[1])))
popDens <- rbind(popDens,totalPop[i]/totalArea[i])
print(city[i])
print(state[i])
print(popDens)
#totalPop <- rbind(totalPop,as.numeric(gsub(",", "", unlist(strsplit(stats[grepl('Land',stats[,1]),2],'sq'))[1])) * popDens[i,])
}
#
#Creates an integer vector of all the contestants' place where he or she finished in their season
place <- as.integer(substring(season.contest$Finish,1,2))
#print(place)
#Creates an integer vector with the number of days a contestant played the game
num.days <- as.integer(gsub("^\\s+", "", substring(season.contest$Finish, nchar(season.contest$Finish)-1, nchar(season.contest$Finish))))
num.days = ifelse(is.na(num.days), 40, num.days)
#print(num.days)
#Creates a data frame with all the extracted contestant information from each season
data.frame(last.name, first.name, age, birth.year, sex, city, state, place,
num.days, season.num = as.integer(i), season.name = seasons[i, 2],
location = seasons[i,3], winner = seasons[i,5], second = seasons[i,6],
third = seasons[i,7], lat = geo[1], lon = geo[2], totalArea,
popDens, totalPop)
}
###Pull data about each of the contestants from all the survivor seasons
for(i in 1:nrow(allseasons)){
print(i)
#create a variable that has the appropriate naming convention to call the webpage for each survivor season
season.name = gsub(" ", "_", allseasons[i, 2])
print(season.name)
#create a html object for each season's wikipedia page
season = read_html(paste("https://en.wikipedia.org/wiki/", season.name))
#creates a variable for which table to look for the contestant table on the Survivor
#Season wikipedia page and then tests if there is an extra table by checking if
#the name "Contestant" is a column header in the table.
table = 2
if(!("Contestant" %in% colnames(season %>%
html_nodes("table") %>%
.[[2]] %>%
html_table(fill=T)))){
table = 3
}
#create a variable called season.contest that hold the table from the wikipedia page with all the contestant's information
contestants = season %>%
html_nodes("table") %>%
.[[table]] %>%
html_table(fill=T)
#Logical check to see if the parse season function is being applied to the first
#season or a subsequent season.
if(i == 1){
allcontestants <- parseSeason(allseasons, contestants, i)
} else {
allcontestants <- rbind(allcontestants, parseSeason(allseasons, contestants, i))
}
}
#Remove duplicate entries for seasons where someone was voted off twice.
allcontestants$name = paste(allcontestants$first.name, allcontestants$last.name)
allcontestants = allcontestants[!duplicated(allcontestants[,c(11,16)]),]
###WORK LEFT TO DO###
###1. Error handle if there is no wikipedia for a location
###2. Calculate number of times played
###3. Scrape bios from webpage
###4. TFIDF Vectorize Bio Text
i = 1
source('~/Documents/Github/Survivor_Winner_Predictor/get_survivor_data.R', echo=TRUE)
#Creates a character vector of the links to the contestants CBS bio pages
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 2
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 3
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
#Creates a character vector of the links to the contestants CBS bio pages
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 4
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 5
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 6
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 7
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 8
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
i = 9
castLinks = getCastLinks(i)
#Creates a character vector of cast bios
bios = getCastBios(castLinks)
print(bios[1])
castLinks
gsub(",(.+)", "", season.contest[,1])
first.name = 'Ami'
last.name = "Cusack"
name = paste(first.name, last.name, sep = '_')
name
links = paste('http://survivor.wikia.com/wiki/',
paste(first.name, last.name, sep = '_'), sep = '')
links
print(links)
source('~/Documents/Github/Survivor_Winner_Predictor/get_survivor_data.R')
links
# download html
htmls <- getURL(links, followlocation = TRUE)
htmls
# parse html
doc = htmlParse(html, asText=TRUE)
source('~/Documents/Github/Survivor_Winner_Predictor/get_survivor_data.R')
#Load packages
library(rvest)
#Load packages
library(rvest)
library(gender)
###Load the data from wikipedia
#Create data frame of survivor seasons
wiki = read_html("https://en.wikipedia.org/wiki/Survivor_(U.S._TV_series)")
allseasons = wiki %>%
html_nodes("table") %>%
.[[2]] %>%
html_table(fill=T)
#Creates a table that contains the U.S.TV ratings information
premier = wiki %>%
html_nodes("table") %>%
.[[4]] %>%
html_table(fill=T)
#Adds the column with the year the season premiered to the allseasons dataframe
premierDate = premier[2:nrow(premier), 3]
if(length(premierDate) < nrow(allseasons)){
allseasons = allseasons[-nrow(allseasons),]
}
allseasons = cbind(allseasons, premierYear = ifelse(substring(premierDate, nchar(premierDate))==']', substring(premierDate, nchar(premierDate)-7, nchar(premierDate)-4), substring(premierDate,nchar(premierDate)-3, nchar(premierDate))))
#Creates the function getLocation which takes the names of a city and state as
#strings and returns a data frame containing the location table from the
#wikipedia page for that city.
getLocation <- function(city = 'Madison', state = 'Wisconsin'){
city = gsub(" ", "_", city)
state = gsub(" ", "_", state)
loc = read_html(paste("https://en.wikipedia.org/wiki/", city,",_", state, sep = ''))
print(loc)
location = loc %>%
html_nodes("table") %>%
.[[1]] %>%
html_table(fill=T)
#print(location)
}
#Creates a function called getGEO which takes in a string from a Wikipedia
#location table and extracts the degree decimal latitude and and longitude
#and returns it as a numeric vector.
getGEO <- function(coordStr){
dd = substring(coordStr,65,83)
return(as.numeric(unlist(strsplit(dd,';'))))
}
getCastLinks <- function(i){
url = paste("http://www.cbs.com/shows/survivor/cast/?season=",i, sep = '')
library('XML')
doc <- htmlParse(url)
links <- data.frame(xpathSApply(doc, "//a/@href"), stringsAsFactors = F)
free(doc)
castLinks = links[grepl('(cast\\/\\d)',links[,1]),1]
return(paste('http://www.cbs.com',castLinks, sep = ''))
}
getCastBios = function(links){
library(RCurl)
# download html
htmls <- getURL(links, followlocation = TRUE)
bios = vector()
for(html in htmls){
# parse html
doc = htmlParse(html, asText=TRUE)
#Extract the class for the cast description
class_xp <- "//div[@class='description']//text()"
bio = xpathSApply( doc,class_xp,xmlValue)
# Replace all \n, \r, \t by spaces
bio = gsub('\\n', ' ', bio)
bio = gsub('\\r', ' ', bio)
bio = gsub('\\t', ' ', bio)
# Join all the elements of the character vector into a single
# character string, separated by spaces
bio = gsub("^\\s+|\\s+$", "", bio)
bio = paste(bio, collapse = ' ')
bios = c(bios, bio)
}
return(bios)
}
#Creates a function to parse all the contestant information from each season's
#wikipedia page
parseSeason <- function(seasons, season.contest, i){
#Creates a string vector of all the contestants' last names for a particular season
last.name <- gsub(",(.+)", "", season.contest[,1])
#Creates a string vector of all the contestants' first names for a particular season
first.name <- gsub("^(.+?), | (.+)","", season.contest[,1])
first.name <- substring(first.name, 1, nchar(first.name)/2)
#Creates a integer vector of all the contestants' age
age <- as.integer(substr(gsub("([^0-9])", "", season.contest[,1]),1,2))
#print(age)
birth.year <- as.integer(as.character(seasons[i,9])) - age
#print(birth.year)
sex <- vector()
for(g in 1:length(first.name)){
if(is.na(birth.year[g])){
sex <- c(sex, "unknown")
}
else if(length(gender(first.name[g], years = birth.year[g], countries = "United States")$gender) == 0){
sex <- c(sex, "unknown")
} else {
sex <- c(sex, gender(first.name[g], years = birth.year[g], countries = "United States")$gender)
}
}
#Creates a string vector of all the contestants' home city
city = gsub("(.+[1-9]), |, (.+)", "", season.contest[,1])
#print(city)
#Creates a string vector of all the contestants' home state
state <- gsub("(.+), ","", season.contest[,1])
#print(state)
geo = data.frame()
totalArea = vector()
totalPop = vector()
popDens = vector()
# for(i in 1:length(city)){
#   tryCatch(getLocation(city[i],state[i]), error = print("no location"))
#   error = function(e) {print(paste("non-numeric argument", x));
#     NaN}
#   location <- getLocation(city[i],state[i])
#   geo <- rbind(geo,getGEO(location[grepl('Coord', location[,2]),1]))
#   stats = location[grepl('km2)', location[,2]),]
#   totalPop = rbind(totalPop,as.numeric(gsub(",", "", location[grepl('[:digits{0-9}:],[:digits{0-9}:]', location[,2]),2][1])))
#   print(totalPop[i])
#   totalArea <- rbind(totalArea,as.numeric(gsub(",", "", unlist(strsplit(stats[grepl('Total',stats[,1]),2],'sq'))[1])))
#   popDens <- rbind(popDens,totalPop[i]/totalArea[i])
#   print(city[i])
#   print(state[i])
#   print(popDens)
#   #totalPop <- rbind(totalPop,as.numeric(gsub(",", "", unlist(strsplit(stats[grepl('Land',stats[,1]),2],'sq'))[1])) * popDens[i,])
# }
# #
#Creates an integer vector of all the contestants' place where he or she finished in their season
place <- as.integer(substring(season.contest$Finish,1,2))
#print(place)
#Creates an integer vector with the number of days a contestant played the game
num.days <- as.integer(gsub("^\\s+", "", substring(season.contest$Finish, nchar(season.contest$Finish)-1, nchar(season.contest$Finish))))
num.days = ifelse(is.na(num.days), 40, num.days)
#Creates a character vector of the links to the contestants CBS bio pages
#castLinks = getCastLinks(i)
links = paste('http://survivor.wikia.com/wiki/',
paste(first.name, last.name, sep = '_'), sep = '')
print(links)
#Creates a character vector of cast bios
bios = getCastBios(links)
print(bios[1])
#Creates a data frame with all the extracted contestant information from each season
data.frame(last.name, first.name, age, birth.year, sex, city, state, place,
num.days, season.num = as.integer(i), season.name = seasons[i, 2],
location = seasons[i,3], winner = seasons[i,5], second = seasons[i,6],
third = seasons[i,7]#,
#lat = geo[1], lon = geo[2], totalArea, popDens, totalPop
)
}
###Pull data about each of the contestants from all the survivor seasons
for(i in 1:nrow(allseasons)){
print(i)
#create a variable that has the appropriate naming convention to call the webpage for each survivor season
season.name = gsub(" ", "_", allseasons[i, 2])
print(season.name)
#create a html object for each season's wikipedia page
season = read_html(paste("https://en.wikipedia.org/wiki/", season.name))
#creates a variable for which table to look for the contestant table on the Survivor
#Season wikipedia page and then tests if there is an extra table by checking if
#the name "Contestant" is a column header in the table.
table = 2
if(!("Contestant" %in% colnames(season %>%
html_nodes("table") %>%
.[[2]] %>%
html_table(fill=T)))){
table = 3
}
#create a variable called season.contest that hold the table from the wikipedia page with all the contestant's information
contestants = season %>%
html_nodes("table") %>%
.[[table]] %>%
html_table(fill=T)
#Logical check to see if the parse season function is being applied to the first
#season or a subsequent season.
if(i == 1){
allcontestants <- parseSeason(allseasons, contestants, i)
} else {
allcontestants <- rbind(allcontestants, parseSeason(allseasons, contestants, i))
}
}
#Remove duplicate entries for seasons where someone was voted off twice.
allcontestants$name = paste(allcontestants$first.name, allcontestants$last.name)
allcontestants = allcontestants[!duplicated(allcontestants[,c(11,16)]),]
links
#Creates a character vector of cast bios
bios = getCastBios(links)
library(RCurl)
# download html
htmls <- getURL(links, followlocation = TRUE)
htmls
# parse html
doc = htmlParse(html, asText=TRUE)
library(XML)
# parse html
doc = htmlParse(html, asText=TRUE)
#Extract the class for the cast description
class_xp <- "//div[@class='mw_headline']//text()"
bio = xpathSApply( htmls,class_xp,xmlValue)
str(htmls)
# download html
htmls <- getURL(links)
htmls
# parse html
doc = htmlParse(htmls, asText=TRUE)
doc
#Extract the class for the cast description
class_xp <- "//div[@class='mw_headline']//text()"
bio = xpathSApply(doc,class_xp,xmlValue)
bio
htmls
links
txt <- htmlToText(links)
library(XML)
txt <- htmlToText(links)
library(RTidyHTML)
# download html
doc.raw <- getURL(links)
doc <- tidyHTML(doc.raw)
install.packages("RTidyHTML")
doc = html(links)
doc = read_html(links)
doc
# download html
doc.raw <- getURL(links)
doc <- read_html(doc.raw)
doc
# parse html
doc = htmlParse(htmls, asText=TRUE)
doc
doc <- read_html(doc.raw)
doc %>%
html_node("profile") %>%
html_text()
doc %>%
html_node("toc") %>%
html_text()
doc %>%
html_node("p") %>%
html_text()
doc %>%
html_node("p:nth-child(7)") %>%
html_text()
doc %>%
html_node("p:nth-child(8)") %>%
html_text()
doc %>%
html_node("p:nth-child(11)") %>%
html_text()
doc %>%
html_node("p:nth-child(7)") %>%
html_text()
source('~/Documents/Github/Survivor_Winner_Predictor/get_survivor_data.R')
doc %>%
html_node("p:nth-child(12)") %>%
html_text()
iris
hist(iris$Sepal.Length)
hist(iris$Sepal.Width)
hist(iris$Petal.Length)
library(MASS)
hist(iris$Petal.Width)
boxcox(iris$Petal.Width)
boxcox(iris)
?boxcox
boxcox(Species ~ Pedal.Width, data = iris)
boxcox(Species ~ Petal.Width, data = iris)
boxcox(Petal.Length ~ Petal.Width, data = iris)
boxcox(Petal.Length ~ ., data = iris)
BoxCox.lambda(iris$Petal.Length, method = c("guerrero", "loglik"), lower = -1, upper = 2)
boxcox.lambda(iris$Petal.Length, method = c("guerrero", "loglik"), lower = -1, upper = 2)
library(ts)
aov(iris$Species ~ iris$Petal.Width)
aov(iris$Petal.Length ~ iris$Species)
iris.aov = aov(iris$Species ~ iris$Petal.Width)
boxcox(iris.aov)
iris.aov = aov(iris$Petal.Length ~ iris$Species)
boxcox(iris.aov)
iris.aov
summary(iris.aov)
boxcox(iris.aov)
hist(iris$Petal.Length)
iris$Petal.Length
hist(iris$Petal.Length+2)
iris.aov = aov(iris$Petal.Length +  2 ~ iris$Species)
boxcox(iris.aov)
bc = boxcox(iris.aov)
max(bc$y)
bc[bc$y == max(bc$y),]
bc[bc$y == max(bc$y)]
bc[,bc$y == max(bc$y)]
bc$y == max(bc$y)
bc[bc$y == max(bc$y),]
str(bc)
data.frame(bc)
bc[bc$y == max(bc$y),]
bc = data.frame(boxcox(iris.aov))
bc[bc$y == max(bc$y),]
bc[bc$y == max(bc$y),]$x
round(bc[bc$y == max(bc$y),]$x,1)
aov.transform = function(df, cat, cont){
df.aov = aov(df[,cont] ~ df[,cat])
bc = data.frame(boxcox(df.aov))
return(round(bc[bc$y == max(bc$y),]$x,1))
}
aov.transform(iris, 'Petal.Length', 'Species')
aov.transform(iris, cat = 'Petal.Length', cont = 'Species')
str(iris)
iris[,'Species']
df = iris
df[,'Species']
cat = 'Species'
df[,cat]
cont = 'Petal.Length'
df[,cont]
aov(df[,cont] ~ df[,cat])
df.aov = aov(df[,cont] ~ df[,cat])
bc = data.frame(boxcox(df.aov))
return(round(bc[bc$y == max(bc$y),]$x,1))
bc = data.frame(boxcox(df.aov))
aov(df[,cont] ~ df[,cat])
df.aov = aov(df[,cont] ~ df[,cat])
bc = data.frame(boxcox(df.aov))
df.aov = aov(df[,cont]+2 ~ df[,cat])
bc = data.frame(boxcox(df.aov))
sample(c('white','african american', 'asian', 'native american', 'pacific islander', 'other'), 5, replace = T, prob = c(.79, .12, .06, .01, .005, .015))
sample(c('white','african american', 'asian', 'native american', 'pacific islander', 'other'), 50, replace = T, prob = c(.79, .12, .06, .01, .005, .015))
race = sample(c('white','african american', 'asian', 'native american', 'pacific islander', 'other'), 1300, replace = T, prob = c(.79, .12, .06, .01, .005, .015))
getwd()
setwd("~/Dropbox/CuylerSchool/R/pay_gap")
write.csv(race, './race.csv')
# Clear workspace
rm(list=ls())
# Load R libraries
library(plyr)
library(ggplot2)
library(knitr)
library(xlsx)
# Turn off scientific notation
options(scipen=999)
# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
str(data)
data$disability = as.factor(data$disability, labels = c('no disability', 'disability'))
data$disability = as.factor(data$disability, levels = c('no disability', 'disability'))
data$disability = as.factor(data$disability)#, levels = c('no disability', 'disability'))
str(data)
data$Veteran = as.factor(data$Veteran)
str(data)
#Remove the unused Duty.Station and Supervisor columns
data = data[,-c(15,16)]
str(data)
# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
#Convert disabilty and veteran columns from numeric to factor
data$disability = as.factor(data$disability)
data$Veteran = as.factor(data$Veteran)
#Remove the unused Duty.Station and Supervisor columns
data = data[,-c(15,16)]
# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
#Convert disabilty and veteran columns from numeric to factor
data$disability = as.factor(data$disability)
data$Veteran = as.factor(data$Veteran)
#Remove any columns that do not have any data
data <- data[,colSums(is.na(df))<nrow(df)]
#Remove any columns that do not have any data
data <- data[,colSums(is.na(data))<nrow(df)]
# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
#Remove any columns that do not have any data
data <- data[,colSums(is.na(data))<nrow(data)]
