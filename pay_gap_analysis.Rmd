---
title: "Sample Pay Gap Analysis in R"
author: "cdo03c"
date: 'January 2018'
output:
  html_document:
    df_print: paged
---


```{r global_options, include=FALSE}
# Clear workspace
rm(list=ls())

# Load R libraries
library(plyr)
library(ggplot2)
library(knitr)
library(xlsx)
library(stargazer)
library(broom)
library(dplyr)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

# Turn off scientific notation 
options(scipen=999)

#Set random seed for reproducibility
set.seed(1234)


# Download the .xslx of the sample pay roll data if it does not exist in the
# working directory
if(!file.exists("./sample_payroll_data.xlsx")){
  download.file(url = "https://github.com/cdo03c/pay_gap/raw/master/sample_payroll_data.xlsx",
                destfile = "./sample_payroll_data.xslx")
}

# Load data
data <- read.xlsx("./sample_payroll_data.xlsx", 1)
```
# Report Scope:

> **This report is a template for conducting and presenting Gender Pay-Gap Analysis in an organization, such as a company or government agency.  This report will address the following questions: Is there a significant difference in income between men and women in this organization? Does the difference vary depending on other factors?**

## Data Summary

To analyze this question, we'll use the [National Longitudinal Survey of Youth](http://www.bls.gov/nls/nlsy97.htm), 1997 cohort data set. This dataset is comprised of about 9000 youth who were initially interviewed in 1997, and then were interviewed more times in the following years. The dataset seeks to produce a longitudinal study of respondents transition from teenage to adult years. This gives us an opportunity to look at how income and gender intersect with other factors, particularly from the teenage years.

```{r, echo=FALSE}
#############################
# Data Cleaning and Prep.
#############################



#Convert disabilty and veteran columns from numeric to factor
data$disability = as.factor(data$disability)
data$veteran = as.factor(data$veteran)

#Remove any columns consisting only of NAs
data <- data[,colSums(is.na(data))<nrow(data)]

# Create five employee age bins.
data$age_bin <- 0
data$age_bin <- ifelse(data$age < 25, 1, data$age_bin) # Below age 25
data$age_bin <- ifelse(data$age >= 25 & data$age < 35, 2, data$age_bin) # Age 25-34.
data$age_bin <- ifelse(data$age >= 35 & data$age < 45, 3, data$age_bin) # Age 35-44.
data$age_bin <- ifelse(data$age >= 45 & data$age < 55, 4, data$age_bin) # Age 45-54.
data$age_bin <- ifelse(data$age >= 55, 5, data$age_bin) # Age 55+.

# Create total compensation variable (base pay + bonus)
data$totalPay <- data$basePay + data$bonus

# Take natural logarithm of compensation variables (for percentage pay gap interpretation in regressions).
data$log_base <- log(data$basePay, base = exp(1)) # Base pay.
data$log_total <- log(data$totalPay, base = exp(1)) # Total comp.
data$log_bonus <- log(data$bonus + 1, base = exp(1)) # Incentive pay. Add 1 to allow for log of 0 bonus values. 

# Create gender dummies (male = 1, female = 0. 
data$male <- ifelse(data$gender == "Male", 1, 0) # Male = 1, Female = 0.

# Check the structure of the imported data.
str(data)

```

```{r}
#############################
# Summary Statistics. 
#############################

# Create an overall table of summary statistics for the data.
summary(data)

# Base pay summary stats.
summary_base <- group_by(data, gender)
summary_base <- summarise(summary_base, meanBasePay = mean(basePay, na.rm = TRUE), medBasePay = median(basePay, na.rm = TRUE), cnt = sum(!(is.na(basePay))) )

# Total pay summary stats.
summary_total <- group_by(data, gender)
summary_total <- summarise(summary_total, meanTotalPay = mean(totalPay, na.rm = TRUE), medTotalPay = median(totalPay, na.rm = TRUE), cnt = sum(!(is.na(totalPay))) )

# Bonus summary stats. 
summary_bonus <- group_by(data,gender)
summary_bonus <- summarise(summary_bonus, meanBonus = mean(bonus, na.rm = TRUE), medBonus = median(bonus, na.rm = TRUE), cnt = sum(!(is.na(bonus))) )

# Performance evaluations summary stats. 
summary_perf <- group_by(data, gender)
summary_perf <- summarise(summary_perf, meanPerf = mean(perfEval, na.rm = TRUE), cnt = sum(!(is.na(perfEval))))

# Departmental distribution of employees.
summary_unit <- group_by(data, organisationUnit, gender)
summary_unit <- summarise(summary_unit, meanTotalPay = mean(totalPay, na.rm = TRUE), cnt = sum(!(is.na(organisationUnit))) ) %>% arrange(desc(organisationUnit, gender))

# Job title distribution of employees (Note: There is a disproportionate number of men in Manager and Software Engineer jobs).
summary_job <- group_by(data, title, gender)
summary_job <- summarise(summary_job, meanTotalPay = mean(totalPay, na.rm = TRUE), cnt = sum(!(is.na(title))) ) %>% arrange(desc(title, gender))
```


<!-- *Note: The most recent survey data is from 2011. Any references to "last year" refer to the year prior to the survey, 2010.* -->

<!-- First we should know the count of respondents, broken down by gender. We have **`r sum(survey$gender=="female")` females and `r sum(survey$gender=="male")` males** in the survey. -->

<!-- Next we'll look at the mean income from last year, broken down by gender.  -->

<!-- ```{r} -->
<!-- gender.income <- aggregate(income.lastyr ~ gender, FUN=mean, na.rm=TRUE, data=survey) -->
<!-- colnames(gender.income) <- c("Gender", "Income") -->
<!-- kable(gender.income) -->
<!-- ``` -->

<!-- We can see here already that the average income from last year for women is lower than it is for men, by $`r round((gender.income[2,2] - gender.income[1,2]), 2)`, or that women are making `r round((100 *(gender.income[1,2])/(gender.income[2,2])), 2)`% of what men make, on average. We'll go into more detail about the statisitical significance of the difference later. -->

<!-- If we look at boxplots of income broken down by gender, we see that the interquartile range for women is lower than that of men, in addition to the lower mean. The outliers for the top earning women catch up with the outliers for the top earning men. -->

<!-- *Note: At this point in the data summary we are excluding the top coded values. The rationale and further analysis regarding top coded values will be explained later in the report.* -->

<!-- ```{r} -->
<!-- qplot(gender, income.lastyr, geom = "boxplot", data = survey.i, fill=gender) + -->
<!--   labs(x="Gender", y="Income Last Year", title="Mean Income by Gender") -->
<!-- ``` -->

<!-- Now that we've looked at the data and observed a difference, we can run a t-test to find the statistical signifigance of this difference. -->

<!-- ```{r} -->
<!-- test.gender <- t.test(survey$income.lastyr ~ survey$gender, conf.level = 0.99) -->
<!-- t.test(survey$income.lastyr ~ survey$gender, conf.level = 0.99) -->
<!-- ``` -->


<!-- At a 95% confidence interval, we find a p-value of `r round(test.gender$p.value, 4)`, indicating that the difference in the means of male and female income are not attributable to random chance. -->

<!-- **So, to begin, we can say that yes, there is a significant difference in income between men and women.** We will now consider the impact of other factors. -->

<!-- #### Race -->
<!-- The mean income from last year, broken down by gender and race, can give us a starting off point for exploring other factors that may contribute to the wage gap. This table displays average income by gender by race, followed by the absolute and then percentage difference for each race in the survey. We can see that the gender wage gap exists for all racial catergories in the survey, though, by varying amounts. Looking at the boxplots, there appears to be less of a difference between the income means by gender for Blacks than for other races. There's a large difference in the means for mixed race people, but there are also only `r nrow(survey[survey$race=="mixed",])` respondends coded as mixed race, making up only `r round(100 * nrow(survey[survey$race=="mixed",])/nrow(survey), 2)`% of the survey. This low sample size makes it difficult to make inferences about this group. -->

<!-- ```{r, results='asis'} -->
<!-- table.race <- with(survey.i, tapply(income.lastyr, INDEX = list(race, gender), FUN = mean, na.rm=TRUE)) -->
<!-- table.race <- transform(table.race, diff = male - female, perc = round(100 * ((female)/(male)), 2)) -->
<!-- table.race <- table.race[order(table.race$perc), ] -->
<!-- colnames(table.race) <- c("Female", "Male", "Abs Diff", "% Diff") -->
<!-- kable(table.race) -->

<!-- # boxplot of race by income -->
<!-- cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") -->
<!-- qplot(race, income.lastyr, data=survey.i, geom="boxplot", fill=gender) + -->
<!--   scale_fill_manual(values=cbPalette) + -->
<!--   labs(x="Race", y="Income Last Year", title="Mean Income by Race & Gender") -->
<!-- ``` -->

<!-- #### Industry -->
<!-- Now we can also look at mean income by gender and industry. Again we see women making less, on average, than men across most of the categories. -->

<!-- *The mean income for women is actually greater than men for `acs special codes`. Similarly to what we saw in the breakdowns by race, though, it is worth nothing that only `acs special codes` made up only `r count(survey$industry=="acs special codes")[2,2]` of respondants, which may mean that a singular or small number of outliers may be skewing this data.* -->

<!-- ```{r} -->
<!-- table.industry <- with(survey.i, tapply(income.lastyr, INDEX = list(industry, gender), FUN = mean, na.rm=TRUE)) -->
<!-- table.industry <- transform(table.industry, diff = male - female, perc = round(100 * ((female)/(male)), 2)) -->
<!-- table.industry <- table.industry[order(table.industry$perc), ] -->
<!-- colnames(table.industry) <- c("Female", "Male", "Abs Diff", "% Diff") -->
<!-- kable(table.industry) -->
<!-- ``` -->

<!-- If we look at boxplots of the distribution of income by gender and industry, we can make some other important observations. It appears that the sample of female active duty military is very small, and if we look at the data we can find it's actually only `r count(survey.female$industry=="active military")[2,2]`. The lower quartile for men in the `mining` and `utilities` are above the upper quartile for women in those industries, while for `agr forest fish` the quartiles don't even overlap. The differences seem smaller for `professional`, `wholesale trade`, `entertain accom food`, and `edu health social`. -->

<!-- ```{r,fig.width=7} -->
<!-- industry.colors <- c("goldenrod2", "#999999") -->
<!-- qplot(industry, income.lastyr, data=survey.i, geom="boxplot", fill=gender) + -->
<!--   scale_fill_manual(values=industry.colors) + -->
<!--   labs(x="Industry", y="Income Last Year", title="Mean Income by Industry & Gender") + -->
<!--   theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1)) -->

<!-- ## NOTE CHANGE: Need to resort by absolute or percent difference -->
<!-- ``` -->



<!-- ## Methodology -->

<!-- #### *Missing Values* -->

<!-- When bringing in and intially coding the data, I excluded missing values from numeric variables. While we can possibly make some assumptions about someone who, for example, did not know their income from last year, when analyzing and computing numeric values it is very difficult to do something with those assumptions. -->

<!-- Unfortuantely, we were missing last year's income data for `r round(100 * (length(which(is.na(survey$income.lastyr))) /length(survey$income.lastyr)), 2)`% of respondents. While that is unfortately a large percentage of our dataset, it still leaves `r length(which(!is.na(survey$income.lastyr)))` respondents, which is a large sample size. The same goes for `industry`, where we were missing `r round(100 * (length(which(is.na(survey$industry))) /length(survey$income.lastyr)), 2)`%, but still have `r length(which(!is.na(survey$industry)))` answers to analyze. -->

<!-- This does introduce a limit into the data, but for the most part the number of missing values was not too great. -->

<!-- For categorical values, things like `valid skip` and `non-interview` were coded into the analysis as `NA`, while in most cases for categorical values `refusal` and `don't know` were coded in as such. The `refusal` and `dont know` values were ignored for some values where they comprised a small sample, but were analyzed further where they comprised a more signifigant proportion of responses.   -->

<!-- #### *Topcoded Values* -->

<!-- For the most part, I removed topcoded values. -->

<!-- One instance where it made a difference was in looking at average income of men and women by industry. The averages by industry were displayed above in the data summary. The table below displays the industry, then mean female and male salary and the absolute difference and percent difference for means, all excluding topcoded values, followed by the absolute and percent differences if you include the topcoded values. The final column finds the difference in percentage points between the means that included the topcoded values and those that did not. This table is sorted by the final column. -->

<!-- ```{r} -->
<!-- ## top coded table -->
<!-- industry.topcode <- with(survey, tapply(income.lastyr, INDEX = list(industry, gender), FUN = mean, na.rm=TRUE)) -->
<!-- industry.topcode <- transform(industry.topcode, diff = male - female, perc = round(100 * ((female)/(male)), 2)) -->
<!-- industry.topcode <- industry.topcode[order(industry.topcode$perc), ] -->

<!-- ## excluding top codes table -->
<!-- table.industry <- with(survey.i, tapply(income.lastyr, INDEX = list(industry, gender), FUN = mean, na.rm=TRUE)) -->
<!-- table.industry <- transform(table.industry, diff = male - female, perc = round(100 * ((female)/(male)), 2)) -->
<!-- table.industry <- table.industry[order(table.industry$perc), ] -->

<!-- ## combining the two tables -->
<!-- table.combine <- transform(table.industry, -->
<!--                            with.diff = industry.topcode$diff, with.perc = industry.topcode$perc, -->
<!--                            differences = table.industry$perc - industry.topcode$perc) -->
<!-- table.combine <- table.combine[order(table.combine$differences), ] -->
<!-- colnames(table.combine) <- c("Female", "Male", "Excld Diff", "Excld % Diff", "W Diff", "W % Diff", "Differences") -->
<!-- kable(table.combine) -->
<!-- ``` -->

<!-- Focusing only on the `Differences` column we can see that some industries -- `info comm` and `construction` in particular at `r table.combine[1,7]`% and `r table.combine[17,7]`% respectively, followed by `active military`, `utilities`, and `mining` -- have high differences depending on the inclusion or exclusion of the topcoded values. -->
<!-- The next question is how big of a difference it makes to our analysis that these values are different. -->

<!-- ```{r} -->
<!-- r2 <- c("info comm", count(survey$industry=="info comm")[2,2],  -->
<!--         round(100 * (count(survey$industry=="info comm")[2,2])/(nrow(survey)), 2)) -->
<!-- r1 <- c("construction", count(survey$industry=="construction")[2,2],  -->
<!--         round(100 * (count(survey$industry=="construction")[2,2])/(nrow(survey)), 2)) -->
<!-- r4 <- c("utilities", count(survey$industry=="utilities")[2,2], -->
<!--         round(100 * (count(survey$industry=="utilities")[2,2])/(nrow(survey)), 2)) -->
<!-- r3 <- c("mining", count(survey$industry=="mining")[2,2], -->
<!--         round(100 * (count(survey$industry=="mining")[2,2])/(nrow(survey)), 2)) -->

<!-- count.industry <- NULL -->
<!-- count.industry <- rbind(count.industry, r1, r2, r3, r4) -->
<!-- colnames(count.industry) <- c("Industry", "Count", "% Respondants") -->
<!-- kable(count.industry) -->
<!-- ``` -->

<!-- Construction workers make up `r count.industry[1,3]`% of the respondents, which is a fair amount, while the number of respondants for other industries comprise a small portion of our sample. -->

<!-- #### *Unexpected Variables That Had No Connection & Other Relationships* -->

<!-- I had expected to find a difference between drug use and income by gender, but it was not very different. -->

<!-- I also thought there may be a difference income by gender based on household income growing up, that wealthier households would possibly set men up to be wealthier to a greater extent than women. However, it appears that greater income as a teenager means greater income as an adult but the difference by gender stays about steady, as seen in this graph below. In order to do this analysis I had to exclude some low, negative household income values that I think may be been erroneously entered. -->

<!-- ```{r} -->
<!-- house.colors <- c("tomato4", "turquoise") -->
<!-- house.plot <- ggplot(data=survey.house.sub, aes(x=household.networth.parent, y=income.lastyr, color=gender)) -->
<!-- house.plot + geom_point() + scale_colour_manual(values=house.colors) + -->
<!--   geom_smooth(method="lm") + -->
<!--   ggtitle("Income by Household Income as Teenager") + -->
<!--   labs(x="Household Income as Teenager", y="Income Last Year")  -->
<!-- ``` -->

<!-- Based on my finding that there is a relationship between weight and the income gap (more below) I suspected there may be a relationship between how respondents evaluated their own weight and income. However, we don't see much difference across different answers to this question other than for men who consider themselves very underweight compared to women in the same catergory. -->

<!-- ```{r} -->
<!-- weight.feel.colors <- c("springgreen3", "#999999") -->
<!-- qplot(weight.2011.feel, income.lastyr, data=survey.weight.sub, geom="boxplot", fill=gender) + -->
<!--   scale_fill_manual(values=weight.feel.colors) + -->
<!--   labs(x="Feelings About Own Weight", y="Income Last Year", title="Mean Income by Feelings About Weight") + -->
<!--   theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1)) -->

<!-- ## NOTE CHANGE: resort by different order -->
<!-- ``` -->



<!-- #### *Chosen Analysis* -->

<!-- I then chose to look at three variables -- `race`, `weight` and `marital status`. As we saw earlier in the data summary, there are observable differences in the pay gap broken down by race, and we can use more finite methods to explore this further. Weight is a great variable in this dataset because of very low missingness. Marital status is interesting because the different factors within the variable impact the predicted incomes -- in some cases increasing the gap and in some cases decreasing the gap, as we'll see further on in the analysis. -->

<!-- ## Findings -->

<!-- To begin, we must know if our data is nearly normal, first looking at the dataset with the topcoded values included: -->

<!-- ```{r}   -->
<!-- with(survey, qqnorm(income.lastyr[gender=="female"])) -->
<!-- with(survey, qqline(income.lastyr[gender=="female"], col="violetred4")) -->
<!-- ``` -->

<!-- and then with the topcoded values removed: -->

<!-- ```{r} -->
<!-- with(survey.i, qqnorm(income.lastyr[gender=="female"])) -->
<!-- with(survey.i, qqline(income.lastyr[gender=="female"], col="violetred4")) -->
<!-- ``` -->

<!-- The fact inclusion of the topcoded values gives us a strange anomoly at the top of the normal Q-Q plot, so we cannot use that for our analysis at this point. While the values when excluding the topcoded values are not perfecly normal, they're close enough that we can run our analyses.  -->

<!-- For starters, I ran a t-test to determine if the difference in average income between men and women is stastistically significant, and with a p-value of `r round((t.test(income.lastyr ~ gender, data = survey.i)$p.value), 2)`, or basically zero, we can say that yes, it is. Based on the coeffients from a linear model, we can predict that, all other variables held constant, a man will make $`r summary(lm(income.lastyr ~ gender, data=survey.i))$coef[2,1]` more per year than a woman. -->

<!-- #### Race -->

<!-- We can use linear regression to explore the collinearity between race and gender. -->

<!-- ```{r} -->
<!-- race.lm <- lm(income.lastyr ~ race + gender, data = survey) -->
<!-- kable(round(summary(race.lm)$coef, 3), format = 'markdown') -->
<!-- race.intercept <- coef(race.lm)[1] -->
<!-- race.hispanic <- coef(race.lm)[2] -->
<!-- race.mixed <- coef(race.lm)[3] -->
<!-- race.white <- coef(race.lm)[4] -->
<!-- race.gendermale <- coef(race.lm)[5] -->
<!-- ``` -->

<!-- Looking at these numbers, we can predict that a given individual makes the income that's at the intercerpt, $`r round(race.intercept ,2)`, for our baseline, which is black females. We can use this test to predict income based on gender and race. Females of other races get the `intercept` plus the coefficent for their race, and males get the `intercept` plus the coefficent for their race and and coefficient for `gendermale`. For example, we predict that a black male makes the baseline, plus the `gendermale` coefficient, which is `r round(race.gendermale, 2)`, so we predict that a black male makes  $`r round((race.intercept + race.gendermale), 2)`. Our model predicts that a white female makes the `intercept` plus the coefficent for white, or $`r round((race.intercept + race.white), 2)`. -->

<!-- ```{r} -->
<!-- black <- subset(survey, survey$race == 'black') -->
<!-- white <- subset(survey, survey$race == 'white') -->
<!-- hispanic <- subset(survey, survey$race == 'hispanic') -->
<!-- mixed <- subset(survey, survey$race == 'mixed') -->
<!-- black.ttest <- t.test(black$income.lastyr ~ black$gender) -->
<!-- white.ttest <- t.test(white$income.lastyr ~ white$gender) -->
<!-- hispanic.ttest <- t.test(hispanic$income.lastyr ~ hispanic$gender) -->
<!-- mixed.ttest <- t.test(mixed$income.lastyr ~ mixed$gender) -->


<!-- ## NOTE: Run two sample t-test to compare white women and black men -->

<!-- ## NOTE: Use race * income as well to find interaction term and then will get black men predicted valued = to black men sample mean. -->
<!-- ``` -->

<!-- We can look at **t-tests of the income differences subsetted by race** to explore this difference further. We find p-values of `r round(black.ttest$p.value, 5)` for black respondants, `r round(white.ttest$p.value, 5)` for white respondants, `r round(hispanic.ttest$p.value, 5)` for hispanic, and `r round(mixed.ttest$p.value, 5)` for mixed race. This higher p-value for the mixed race catergory reflects our initial assumption that while graphically there appears to be a very large difference in these means, the small sample size may be skewing that data. We also see a slightly higher p-value for the black pay gap than for white or hispanic, confirming our earlier inferences that the gap for blacks is not as significant.  -->



<!-- #### Weight -->

<!-- It appears that as men weight more, we can predict that they make slightly more money, while we predict that heavier women make less, as we can see in this linear regression. -->

<!-- *The data here has been subsetted to eliminated some extremely low and extremely high values that I am assuing were entry errors.* -->

<!-- ```{r} -->
<!-- weight.colors <- c("tomato2", "#999999") -->
<!-- weight.plot <- ggplot(data=survey.weight.sub, aes(x=weight.2011, y=income.lastyr, color=gender)) -->
<!-- weight.plot + geom_point() + scale_colour_manual(values=weight.colors) + -->
<!--   geom_smooth(method="lm") + -->
<!--   #stat_smooth() + -->
<!--   ggtitle("Income by Weight") + -->
<!--   labs(x="Weight in 2011",y="Income Last Year")  -->
<!-- ``` -->

<!-- The findings appear to be similar to the weights that were reported in 2004, so it is not worth doing a separate anaysis of these values. -->

<!-- ```{r} -->
<!-- weight.colors <- c("tomato2", "#999999") -->
<!-- weight.2004.plot <- ggplot(data=survey.weight.sub2004, aes(x=weight.2004, y=income.lastyr, color=gender)) -->
<!-- weight.2004.plot + geom_point() + scale_fill_manual(values=weight.colors) + -->
<!--   geom_smooth(method="lm") + -->
<!--   ggtitle("Income by Weight") + -->
<!--   labs(x="Weight in 2004",y="Income Last Year")  -->

<!-- # NOTE: -->
<!-- # [**Possible additional research: Comparision of weight, per respondant, between 2004 and 2011**] -->
<!-- # **take out linear and it might find a curve or something instead, just stat_smooth(), not lm** -->
<!-- # **maybe fit polynomial** -->
<!-- # **use different model if better fit** -->
<!-- # **take out lower weights, up to maybe 95lb**] -->
<!-- ``` -->


<!-- Based on looking at the regression model, I hypothesized that weight is a factor that impacts the difference in income between men and women. I then used a linear model to find a more precise prediction. -->

<!-- ```{r} -->
<!-- income.weight.lm <- lm(income.lastyr ~ gender * weight.2011, data = survey.weight.sub) -->
<!-- #income.weight.lm$coef -->
<!-- kable(round(summary(income.weight.lm)$coef, 3), format = 'markdown') -->

<!-- ## setting up variables for ANOVA -->
<!-- gender.lm <- (lm(income.lastyr ~ gender, data=survey.weight.sub)) -->
<!-- weight.lm <- (lm(income.lastyr ~ gender * weight.2011, data=survey.weight.sub)) -->
<!-- ``` -->

<!-- Unexpectedly, this model produces a negative coefficent for `gendermale`. There were a small number of very small values that skewed the data, including some negative values, and we will ignore these outliers. Either way, for every pound heavier that a man is, we can predict he will make $`r round((coef(income.weight.lm)["weight.2011"] + coef(income.weight.lm)[4]),2)` more (the `weight.2011` coefficient plus the `gendermale:weight.2011` coefficient), but for every pound heavier that a woman is, we predict she'll make $`r round(coef(income.weight.lm)["weight.2011"], 2)` (just the `weight.2011` coefficient). -->

<!-- To give an example, if we look at a 180lb man and a 180lb woman and hold all other variables constant, we predict he'll make the `intercept` plus the coefficient for `gendermale` + (180lb) * (`weight.2011` + `gendermale:weight.2011`), or $`r round((coef(income.weight.lm)[1] + coef(income.weight.lm)[2] + 180 * (coef(income.weight.lm)[3] + coef(income.weight.lm)[4])), 2)`. We predict a woman of the same weight will make the `intercept` + her weight, 180lb * `weight.2011`, or $`r round((coef(income.weight.lm)[1] + 180 * (coef(income.weight.lm)[3])), 2)`. -->

<!-- Lastly, we should test if our model using weight and gender is signifigantly more predictive than the model using gender alone. Running an ANOVA comparing the two models we find a p-value of `r round((anova(gender.lm, weight.lm)[2,6]), 2)`, we can say that yes, it is a signifigantly better model. -->

<!-- To look at the data even more precisely, let's explore non-linear options for models -->
<!-- . -->

<!-- ```{r} -->
<!-- weight.colors <- c("tomato2", "#999999") -->
<!-- weight.plot <- ggplot(data=survey.weight.sub, aes(x=weight.2011, y=income.lastyr, color=gender)) -->
<!-- weight.plot + geom_point(size = 1.5, alpha = 0.6) + -->
<!--   scale_colour_manual(values=weight.colors) + -->
<!--   #geom_smooth(method="lm") + -->
<!--   stat_smooth() + -->
<!--   ggtitle("Income by Weight") + -->
<!--   labs(x="Weight in 2011",y="Income Last Year")  -->
<!-- ``` -->

<!-- We can see that while female respondants relationship between weight and income last year maintains a fairly linear relationship, there is more variablity in the curve of the relationship for men. An area for further research may be to try to normalize this data against average weights for men and women and then look at the relationship between income last year and weight relative to average rate for the gender. -->

<!-- #### Marital Status -->

<!-- Next we'll look at the impact of marital status on income. First, though, we should check the size of the values before moving further in our analysis. -->

<!-- ```{r} -->
<!-- r4 <- c("divorced", count(survey$marital.status=="divorced")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="divorced")[2,2])/(nrow(survey)), 2)) -->
<!-- r2 <- c("married", count(survey$marital.status=="married")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="married")[2,2])/(nrow(survey)), 2)) -->
<!-- r1 <- c("never married", count(survey$marital.status=="never married")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="never married")[2,2])/(nrow(survey)), 2)) -->
<!-- r5 <- c("separated", count(survey$marital.status=="separated")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="separated")[2,2])/(nrow(survey)), 2)) -->
<!-- r7 <- c("widowed", count(survey$marital.status=="widowed")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="widowed")[2,2])/(nrow(survey)), 2)) -->
<!-- r3 <- c("NA", count(survey$marital.status=="NA")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="NA")[2,2])/(nrow(survey)), 2)) -->
<!-- r6 <- c("invalid skip", count(survey$marital.status=="invalid skip")[2,2],  -->
<!--         round(100 * (count(survey$marital.status=="invalid skip")[2,2])/(nrow(survey)), 2)) -->

<!-- count.marital <- NULL -->
<!-- count.marital <- rbind(count.marital, r1, r2, r3, r4, r5, r6, r7) -->
<!-- colnames(count.marital) <- c("Industry", "Count", "% Respond") -->
<!-- kable(count.marital) -->

<!-- ## NOTE CHANGE: **table of data and then another colum of table(my.data)/ nrows(my.data) and then append as another column to table, cbind** -->
<!-- ``` -->


<!-- We can see that we have a very small sample size of `widowed` respondents, and upon looking at the data can see it is only `r count(survey.i$marital.status=="widowed")[2,2]`. Based on this sample size, we cannot make much inferences about widowed people from this data. There are also a small, but much larger number of `separated` individuals. We should keep this size in mind as we move through the analysis. We are missing values for `r as.numeric(count.marital[3,3]) - as.numeric(count.marital[6,3])`% of respondents (`NA` plus `invalid skip`), but that is not so low that we should not continue with the analysis. -->

<!-- So now let's look at income by gender and marital status. -->

<!-- ```{r} -->
<!-- marital.colors <- c("brown2", "cornsilk3") -->
<!-- qplot(marital.status, income.lastyr, data=survey.i, geom="boxplot", fill=gender) + -->
<!--   scale_fill_manual(values=marital.colors) + -->
<!--   labs(x="Marital Status", y="Income Last Year", title="Mean Income by Marital Status & Gender") + -->
<!--   theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))  -->
<!-- ``` -->

<!-- ```{r} -->
<!-- weight.colors <- c("pink", "#999999") -->
<!-- weight.plot <- ggplot(data=survey.weight.sub, aes(x=weight.2011, y=income.lastyr, color=gender)) -->
<!-- weight.plot + geom_point() + scale_colour_manual(values=weight.colors) + -->
<!--   geom_smooth(method="lm") + -->
<!--   ggtitle("Income by Weight") + -->
<!--   labs(x="Weight in 2011",y="Income Last Year")  -->

<!-- ``` -->

<!-- From this plot we can see that there appears to be the smallest difference between men and women who have never been married and the greatest difference between those who are separated, and a somewhat similar gender wage gap between those who are married or divorced. -->

<!-- ```{r} -->
<!-- income.marital.lm <- lm(income.lastyr ~ gender * marital.status, data = survey.i) -->
<!-- #summary(income.marital.lm) -->
<!-- #income.marital.lm$coef -->
<!-- kable(round(summary(income.marital.lm)$coef, 3), format = 'markdown') -->

<!-- ## setting up variables for ANOVA -->
<!-- gender.lm <- (lm(income.lastyr ~ gender, data=survey.i)) -->
<!-- marital.lm <- (lm(income.lastyr ~ gender * marital.status, data=survey.i)) -->
<!-- aov.out <- anova(gender.lm, marital.lm) -->
<!-- pvals <- aov.out[["Pr(>F)"]] -->

<!-- ## setting up for comparison -->
<!-- male.married <- (coef(income.marital.lm)[1] + coef(income.marital.lm)[2] + coef(income.marital.lm)[4] + coef(income.marital.lm)[9]) -->
<!-- female.married <- (coef(income.marital.lm)[1] + coef(income.marital.lm)[4]) -->
<!-- male.nevermarried <- (coef(income.marital.lm)[1] + coef(income.marital.lm)[2] + coef(income.marital.lm)[5] + coef(income.marital.lm)[10]) -->
<!-- female.nevermarried <- (coef(income.marital.lm)[1] + coef(income.marital.lm)[5]) -->
<!-- stat.married <- coef(income.marital.lm)[4] -->

<!-- stat.nevermarried <- coef(income.marital.lm)[5] -->
<!-- stat.sep <- coef(income.marital.lm)[6] -->
<!-- men.married <- coef(income.marital.lm)[9] -->
<!-- men.nevermarried <- coef(income.marital.lm)[10] -->
<!-- men.sep <- coef(income.marital.lm)[11] -->
<!-- men <- coef(income.marital.lm)[2] -->
<!-- mar.intercept <- coef(income.marital.lm)[1] -->

<!-- men1 <- (men) + (men.married) -->
<!-- men2 <- (men) + (men.nevermarried) -->
<!-- men3 <- (men) + (men.sep) -->

<!-- married.diff.p <- round((100 * (female.married)/(male.married)), 2) -->
<!-- nevermarried.diff.p <- round((100 * (female.nevermarried)/(male.nevermarried)), 2) -->
<!-- gender.diff.p <- round((100 * (gender.income[1,2])/(gender.income[2,2])), 2) -->

<!-- ``` -->

<!-- The baseline for this model is `divorced` and `female`. We can see from these coefficients that we predict that compared to a divorced woman, women who are married make $`r round(stat.married, 2)` more, never married women make $`r round(stat.nevermarried, 2)` more,  while separated women make $`r round(stat.sep, 2)`.  -->

<!-- On the other hand, for men we predict that compared to a divorced man, married men make $`r round(men1, 2)` more, never married men make $`r round(men2, 2)` and separated men make $`r round(men3, 2)`. -->

<!-- Holding all other variables constant, if we compared a married man and a married woman, we'd predict that he'd make : -->
<!-- `intercept` + `gendermale` + `marital.statusmarried` + `gendermale:marital.statusmarried` = $`r round(male.married, 2)` -->
<!-- and predict that she'd make: -->
<!-- `intercept` + `marital.statusmarried` = $`r round(female.married, 2)` -->

<!-- This difference -- $`r round((male.married - female.married), 2)`, with women making `r round(married.diff.p, 2) ` % of men's mean income, is lower than what we found the overall mean difference (`r gender.diff.p`%). -->


<!-- Holding all other variables constant, if we compared a never married man and a never married woman, we'd predict that he'd make : -->
<!-- `intercept` + `gendermale` + `marital.statusnever married` + `gendermale:marital.statusnever married` = $`r round(male.nevermarried, 2)` -->
<!-- and predict that she'd make: -->
<!-- `intercept` + `marital.statusnever married` = $`r round(female.nevermarried, 2)` -->

<!-- This difference here, $`r round((male.nevermarried - female.nevermarried), 2)`, with women making `r nevermarried.diff.p` % of men's mean income, is a much closer percentage than what we found the overall mean difference (`r gender.diff.p`%). -->

<!-- Since we found two different amounts in comparison to our intial findings on the wage gap, it seems reasonable that marital status has a further impact on that gap -- that depending on a woman's marital status she may make even less than similar man or may make closer -- so this variable is an important predictor. -->

<!-- As we did with the `weight` variable, let's check that our `marital status` analysis gives a closer prediction than the general model. Running an ANOVA comparing the two models we find a p-value of `r round(pvals[2], 8)`, we can say that yes, it is a signifigantly better model. -->

<!-- ```{r} -->
<!-- anova(gender.lm, marital.lm) -->
<!-- ``` -->

<!-- This analysis was done when excluding topcoded variables, so let's see if it's different with the topcoded valudes in. -->

<!-- ```{r} -->
<!-- ########################################################################################### -->
<!-- # Model Estimation: OLS with controls.  -->
<!-- # Coefficient on "male" has the interpretation of approximate male pay advantage ("gender pay gap"). -->
<!-- ########################################################################################### -->

<!-- ############################# -->
<!-- # Logarithm of Base Pay -->
<!-- ############################# -->

<!-- # No controls. ("unadjusted" pay gap.) -->
<!-- model1 <- lm(log_base ~ male, data = data) -->
<!-- summary(model1) -->

<!-- # Adding "human capital" controls (performance evals, age and education). -->
<!-- model2 <- lm(log_base ~ male + perfEval + age_bin + edu, data = data) -->
<!-- summary(model2) -->

<!-- # Adding all controls. ("adjusted" pay gap.) -->
<!-- model3 <- lm(log_base ~ male + perfEval + age_bin + edu + dept + seniority + title, data = data) -->
<!-- summary(model3) -->

<!-- # Print log base pay "adjusted" gender pay gap and p-value. -->
<!-- logbase_pay_gap <- coef(model3)["male"] # male coefficient for adjusted pay gap. -->
<!-- logbase_pay_pvalue <- coef(summary(model3))["male", "Pr(>|t|)"] # associated p value.  -->
<!-- print(logbase_pay_gap) -->
<!-- print(logbase_pay_pvalue) -->

<!-- # Publish HTML stargazer table of regression results. -->
<!-- stargazer(model1, model2, model3, type = "html", out = "results.htm", omit = c("title", "dept", "edu"), -->
<!--             add.lines = list( -->
<!--               c("Controls:"), -->
<!--               c("Education","No","Yes", "Yes"), -->
<!--               c("Department","No", "No", "Yes"), -->
<!--               c("Job Title", "No", "No", "Yes"), -->
<!--               c("-----") ) -->
<!--           ) -->


<!-- ############################# -->
<!-- # Results by Department -->
<!-- # (Interaction of male x dept) -->
<!-- # To test for differences by department, examine significance of each "male x dept" coefficient. -->
<!-- # For the gender pay gap by department, add the "male" + "male x dept" coefficients from this model.  -->
<!-- ############################# -->

<!-- # All controls with department interaction terms.  -->
<!-- dept_results <- lm(log_base ~ male*dept + perfEval + age_bin + edu + seniority + title, data = data) -->
<!-- summary(dept_results) -->
<!-- dept_results_clean <- tidy(dept_results) # Produce clean file of regression results without "*". -->

<!-- # Publish HTML stargazer tables of regression results.  -->
<!-- stargazer(dept_results, type = "html", out = "dept.htm", omit = c("title", "edu")) -->
<!-- write.csv(dept_results_clean, file = "dept_clean.csv") # -->



<!-- ############################# -->
<!-- # Results by Job Title  -->
<!-- # (Interaction of male x job title)  -->
<!-- # To test for differences by job title, examine significance of each "male x job title" coefficient. -->
<!-- # For the gender pay gap by job title, add the "male" + "male x job title" coefficients from this model.  -->
<!-- ############################# -->

<!-- # All controls with job title interaction terms.  -->
<!-- job_results <- lm(log_base ~ male*title + perfEval + age_bin + edu + seniority + dept, data = data) -->
<!-- summary(job_results) -->
<!-- job_results_clean <- tidy(job_results) # Produce clean file of regression results without "*". -->

<!-- # Push out HTML stargazer tables.  -->
<!-- stargazer(job_results, type = "html", out = "job.htm", omit = c("department", "edu")) -->
<!-- write.csv(job_results_clean, "job_clean.csv") -->

<!-- # For additional analysis via Oaxaca-Blinder decomposition, please see documentation for the "oaxaca" package in R. -->
<!-- # https://cran.r-project.org/web/packages/oaxaca/index.html -->

<!-- ``` -->

<!-- ## Discussion -->

<!-- It appears that both `weight` and `marital status` are statistically signifigant in contributing to the wage gap between men and women. -->

<!-- The weight variable is interesting. It confirms what I and others suspect and has been proven in some studies -- women are peanlized for their appearance to a greater extent than men are, and in particular for gaining weight. I was surprised, though, that a person's self-assesment of being overweight, underweight, or average did not seem ot have a reltaionship to income. -->

<!-- It's also interesting that when comparing men and women who had never been married the wage gap closed much tighter. While this data only looked at a small range of birth years, and therefore a small range of ages, it might be worth further investigating if other factors, such as age, contribute to this difference. -->

<!-- While this analysis is fairly accurate based on the data presented, and it was on a fairly large dataset, this is still only one dataset. In order to feel more confident in the findings I would want to run similar tests using other large, longitudinal datsets to see if the same findings stand. -->

<!-- Parts of the code in this document were borrowed from the following sources: -->
<!-- https://glassdoor.app.box.com/v/gender-pay-code -->
<!-- https://github.com/laurenrenaud/r-projects/blob/master/gender-gap.rmd -->

